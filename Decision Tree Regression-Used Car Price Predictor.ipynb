{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree regression-Used Car Price Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine,inspect, func\n",
    "from config import password\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "from collections import Counter\n",
    "import math \n",
    "from scipy import stats\n",
    "import datetime\n",
    "import matplotlib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from spark_sklearn import GridSearchCV\n",
    "from spark_sklearn.util import createLocalSparkSession\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the sql database\n",
    "connection_string = \"postgres:\"+password+\"@localhost:5432/hondadb\"\n",
    "engine = create_engine(f'postgresql://{connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_sql(\"SELECT * FROM cleanedcardb2\",\n",
    "                     con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Car Type', 'Model']\n",
    "les = {}\n",
    "\n",
    "for f in features:\n",
    "  les[f] = preprocessing.LabelEncoder()\n",
    "  les[f] = les[f].fit(df_cleaned[f])\n",
    "  df_cleaned[f] = les[f].transform(df_cleaned[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets\n",
    "train_set, test_set = train_test_split(df_cleaned, test_size = 0.3, random_state = 100)\n",
    "\n",
    "# Separating target labels from the rest\n",
    "df_train = train_set.drop(\"Price\", axis=1) #train without target\n",
    "df_price_train = train_set[\"Price\"].copy() #target\n",
    "\n",
    "df_test  = test_set.drop(\"Price\", axis=1)\n",
    "df_price_test = test_set[\"Price\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the best score achieved by the model over all the cv splits\n",
    "def best_score(forest, cv):\n",
    "  best_score = 0\n",
    "  for i in range(0, cv):\n",
    "    items = list(map(lambda x: abs(x), forest.cv_results_['split'+str(i)+'_test_score']))\n",
    "    arr = np.append(best_score, items)\n",
    "    best_score = max(arr)\n",
    "  \n",
    "  return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Param Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions returns the best combination of parameters, which allows to\n",
    "# get the best score\n",
    "def best_params(forest):\n",
    "  return forest.cv_results_['params'][forest.cv_results_['rank_test_score'][0]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true (y_true) and predicted (y_predict) values based on the metric chosen. \"\"\"\n",
    "    \n",
    "    score = r2_score(y_true, y_predict)\n",
    "    \n",
    "    # Return the score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def DT_SparkizedGridSearchCV(X, y):\n",
    "    \"\"\" Performs grid search over the 'max_depth' parameter for a \n",
    "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 42)\n",
    "\n",
    "    # Create a decision tree regressor object\n",
    "    regressor = DecisionTreeRegressor()\n",
    "\n",
    "    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "    params = {'max_depth':[1, 5, 10, 15, 16, 17]}\n",
    "\n",
    "    # Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "\n",
    "    # Create the grid search cv object --> GridSearchCV()\n",
    "    sc = createLocalSparkSession().sparkContext\n",
    "    grid = GridSearchCV(sc, estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    tree_reg = grid.fit(X, y)\n",
    "    \n",
    "    # Return grid search output after fittig the data\n",
    "    return tree_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 'max_depth' is 17 for the optimal model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Fit the training data to the model using spark parallelized grid search CV\n",
    "tree_reg = DT_SparkizedGridSearchCV(df_train, df_price_train)\n",
    "\n",
    "# Takign best parameters\n",
    "bp = best_params(tree_reg)\n",
    "\n",
    "# Produce the optimal value for 'max_depth'\n",
    "print(\"Parameter 'max_depth' is {} for the optimal model.\".format(bp['max_depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 140 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=17, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Due to the limitation of the spark-sklearn library's implementation of\n",
    "GridSearchCV, best_estimator_ parameter it's not available, so we need to\n",
    "fit a DecisionTreeRegressor on the best parameters given to us by gridSearchCV\n",
    "\"\"\"\n",
    "tree_reg_model = DecisionTreeRegressor(\n",
    "                              max_depth=bp['max_depth'])\n",
    "%time tree_reg_model.fit(df_train, df_price_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree Regressor parameters:\n",
      "{'max_depth': 17}\n",
      "\n",
      "Decision Tree Regressor score without CV on train set: 0.977\n",
      "Decision Tree Regressor score without CV on test set: 0.841\n",
      "Decision Tree Regressor Best score with CV=10: 0.853\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeRegressor score for price prediction\n",
    "\n",
    "\n",
    "print(\"Best Decision Tree Regressor parameters:\")\n",
    "print(bp)\n",
    "print(\"\\nDecision Tree Regressor score without CV on train set: %.3f\" % tree_reg_model.score(df_train, df_price_train)) #score on train set\n",
    "print(\"Decision Tree Regressor score without CV on test set: %.3f\" % tree_reg_model.score(df_test, df_price_test)) # score on test set\n",
    "print(\"Decision Tree Regressor Best score with CV=10: %.3f\" % best_score(tree_reg, 10)) # -> best score on test set is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2615.4821147819575"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction on whole training set\n",
    "price_predictions_train = tree_reg_model.predict(df_train) #using the whole training set for making prediction with the final model given by the best CV parameters\n",
    "\n",
    "# Reversing np.log operation\n",
    "price_predictions_train_normal = np.exp(price_predictions_train)\n",
    "df_price_train_normal = np.exp(df_price_train)\n",
    "\n",
    "# MSE between target values (i.e known) and predicted values\n",
    "lin_mse = mean_squared_error(df_price_train_normal, price_predictions_train_normal)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse # is higher than RMSE of linear regression, in fact the best score is smaller (0.58 vs 0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5900.         49487.         21990.         13920.81628693\n",
      " 76999.         33590.8275777   7000.         36887.63546117\n",
      " 13995.         24645.16597243]\n",
      "\n",
      "\n",
      "[5899.999999999999, 49486.999999998035, 21990.000000000084, 14995.000000000002, 76998.99999999984, 34498.000000000866, 6999.999999999999, 40278.99999999843, 13994.999999999933, 20883.999999999953]\n"
     ]
    }
   ],
   "source": [
    "print(price_predictions_train_normal[10:20])\n",
    "print('\\n')\n",
    "print(list(df_price_train_normal[10:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7302.2391917535515"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "price_predictions_test = tree_reg_model.predict(df_test)\n",
    "\n",
    "# reversing np.log operation\n",
    "price_predictions_test_normal = np.exp(price_predictions_test)\n",
    "df_price_test_normal = np.exp(df_price_test)\n",
    "\n",
    "final_mse = mean_squared_error(df_price_test_normal, price_predictions_test_normal)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13395.97122091 13995.         13832.32661973 18822.5249439\n",
      " 29798.         41495.         25761.66283808 22416.29677533\n",
      " 39378.93952429 16188.        ]\n",
      "\n",
      "\n",
      "[17994.999999999993, 11499.999999999976, 11994.99999999993, 23500.00000000118, 26887.9999999989, 28999.9999999997, 29189.000000000244, 21488.00000000001, 38952.99999999941, 13994.999999999933]\n"
     ]
    }
   ],
   "source": [
    "print(price_predictions_test_normal[10:20]) #predictions on test set\n",
    "print('\\n')\n",
    "print(list(df_price_test_normal[10:20])) #known values in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181936758800683"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(df_price_test_normal, price_predictions_test_normal, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model for type prediction\n",
    "pickle.dump(tree_reg_model, open(\"tree_reg_model_final.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model for type prediction\n",
    "tree_reg_model = pickle.load(open(\"tree_reg_model_final.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ 6738.56612956  6354.28346818  8677.1354204   7599.20352357\n",
      "  7323.8271213   8594.16815341 11112.69258222  7930.23587412\n",
      "  7248.9485119   7709.18531306]\n",
      "Mean: 7928.824609771776\n",
      "Standard deviation: 1265.729369372243\n"
     ]
    }
   ],
   "source": [
    "# Cross val score on training set\n",
    "\n",
    "train_scores = cross_val_score(tree_reg_model, df_train, np.exp(df_price_train),\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-train_scores)\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.8454277  0.85544297 0.79057655 0.79828759 0.80145012 0.68641467\n",
      " 0.72617049 0.78371573 0.81920005 0.80746474]\n",
      "Mean: 0.7914150612457654\n",
      "Standard deviation: 0.048514317927850596\n"
     ]
    }
   ],
   "source": [
    "train_scores = cross_val_score(tree_reg_model, df_train, np.exp(df_price_train),\n",
    "                          cv=10)\n",
    "\n",
    "display_scores(train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
